WEBVTT

00:00:00.001 --> 00:00:03.840
Hi everyone, my name is Justin from the

00:00:03.840 --> 00:00:06.000
University of LiÃ¨ge in Belgium.

00:00:06.000 --> 00:00:09.270
Today in this talk I'm going to introduce you to

00:00:09.270 --> 00:00:12.000
something that we have called cross-layer telemetry.

00:00:12.000 --> 00:00:16.920
So basically we try to find a name as explicit as

00:00:16.920 --> 00:00:20.610
possible and I will go much into detail about

00:00:20.610 --> 00:00:21.000
this.

00:00:21.000 --> 00:00:25.000
But first let me remind you some basics.

00:00:25.000 --> 00:00:28.130
So we moved from a kind of monolithic

00:00:28.130 --> 00:00:31.000
architecture to microservices.

00:00:31.000 --> 00:00:35.000
So you can see microservices everywhere now.

00:00:35.000 --> 00:00:39.000
And there are a lot of reasons for that.

00:00:39.000 --> 00:00:42.280
Mainly that it's easier and faster to deploy, to

00:00:42.280 --> 00:00:48.000
maintain, but there are also other reasons.

00:00:48.000 --> 00:00:51.580
So if you look at this kind of architecture and

00:00:51.580 --> 00:00:55.000
let's assume there is a problem somewhere

00:00:55.000 --> 00:00:58.570
and I ask you to debug and to find the problem

00:00:58.570 --> 00:01:01.000
here, you would tell me "okay, easy game".

00:01:01.000 --> 00:01:05.470
But what about this one? So it gets a little bit

00:01:05.470 --> 00:01:07.000
more complicated.

00:01:07.000 --> 00:01:11.710
So hopefully for that kind of spaghetti microservice

00:01:11.710 --> 00:01:13.000
architecture,

00:01:13.000 --> 00:01:19.000
you have application management performance,

00:01:19.000 --> 00:01:19.840
which is actually application performance

00:01:19.840 --> 00:01:20.000
management,

00:01:20.000 --> 00:01:24.200
and in this case, more specifically, you have

00:01:24.200 --> 00:01:27.000
distributed tracing tools.

00:01:27.000 --> 00:01:32.110
In this talk I will take Jaeger as an example,

00:01:32.110 --> 00:01:33.880
but you have to know that there are a lot of

00:01:33.880 --> 00:01:36.000
other alternatives.

00:01:36.000 --> 00:01:42.000
Jaeger is just a famous one in all of them.

00:01:42.000 --> 00:01:45.420
So such tool is very useful when we are dealing

00:01:45.420 --> 00:01:47.000
with microservices.

00:01:47.000 --> 00:01:49.000
And they all have something in common.

00:01:49.000 --> 00:01:53.110
They all have the same notion, the same concept

00:01:53.110 --> 00:01:55.000
of trace and spans.

00:01:55.000 --> 00:01:58.340
So you can see a trace as a hierarchy of trace

00:01:58.340 --> 00:01:59.000
and spans.

00:01:59.000 --> 00:02:03.000
So a trace can contain subtraces and also spans.

00:02:03.000 --> 00:02:07.000
And you can see a span as a unit of work, of code.

00:02:07.000 --> 00:02:10.800
So this is just a part of your code that you want

00:02:10.800 --> 00:02:12.000
to monitor.

00:02:12.000 --> 00:02:14.860
And here at the bottom of the slide, you can see

00:02:14.860 --> 00:02:18.000
a screenshot of the Jaeger visualization.

00:02:18.000 --> 00:02:21.440
So you have basically a main trace, which

00:02:21.440 --> 00:02:23.000
contains two subtraces,

00:02:23.000 --> 00:02:27.000
and each trace has some child span.

00:02:27.000 --> 00:02:30.370
So here a span, and then a span, a child span,

00:02:30.370 --> 00:02:31.000
etc.

00:02:31.000 --> 00:02:34.540
So you can see on the left that you have a clear

00:02:34.540 --> 00:02:38.000
visualization of the flow of the code path.

00:02:38.000 --> 00:02:41.400
And on the right, you have the time taken by each

00:02:41.400 --> 00:02:42.000
part.

00:02:42.000 --> 00:02:46.000
So it's very useful for those two reasons.

00:02:46.000 --> 00:02:49.560
You can see, as I said, the path taken by your

00:02:49.560 --> 00:02:50.000
code.

00:02:50.000 --> 00:02:52.920
So if you execute this one, what is going to be

00:02:52.920 --> 00:02:55.000
triggered, etc., etc.

00:02:55.000 --> 00:02:58.000
And you can see the time that it took.

00:02:58.000 --> 00:03:02.000
But such tool is limited to layer 5 and above.

00:03:02.000 --> 00:03:08.190
So if you have a problem, a low-level issue, so a

00:03:08.190 --> 00:03:10.000
network issue, for instance,

00:03:10.000 --> 00:03:14.000
well, that's when you're facing a problem.

00:03:14.000 --> 00:03:19.290
And just to cite an example, let's assume that my

00:03:19.290 --> 00:03:21.000
database lookup is slow.

00:03:21.000 --> 00:03:24.180
So I can see in Jaeger that my database lookup is

00:03:24.180 --> 00:03:26.000
slow because I traced it.

00:03:26.000 --> 00:03:29.180
In that case, maybe should I just blame the app,

00:03:29.180 --> 00:03:32.240
or should I put the blame on the server, or even

00:03:32.240 --> 00:03:33.000
the database?

00:03:33.000 --> 00:03:36.100
Or maybe this is a network issue, or actually it

00:03:36.100 --> 00:03:38.000
can be anything else.

00:03:38.000 --> 00:03:42.000
So it's hard to know exactly what it is.

00:03:42.000 --> 00:03:45.420
So you don't know why, and you don't even know

00:03:45.420 --> 00:03:48.000
where exactly the problem is.

00:03:48.000 --> 00:03:51.980
So the only thing that you know is what Jaeger

00:03:51.980 --> 00:03:56.000
tells you, that it's slow. That's all.

00:03:56.000 --> 00:04:00.000
So let's just see a basic, simple topology here.

00:04:00.000 --> 00:04:03.330
So you have the app, the database, and this guy

00:04:03.330 --> 00:04:05.780
in the middle of the path with a congestion of

00:04:05.780 --> 00:04:08.000
one of its interfaces.

00:04:08.000 --> 00:04:12.180
And so Jaeger will just report a slow execution

00:04:12.180 --> 00:04:13.000
time.

00:04:13.000 --> 00:04:16.520
So when you see this, you will investigate on the

00:04:16.520 --> 00:04:19.000
app, you will investigate on the database,

00:04:19.000 --> 00:04:22.000
and actually you wouldn't find anything.

00:04:22.000 --> 00:04:25.950
So you would be left scratching your head and

00:04:25.950 --> 00:04:29.000
wondering why it takes so long.

00:04:29.000 --> 00:04:32.870
And that's a big problem because root cause

00:04:32.870 --> 00:04:36.000
analysis, it's hard in this case.

00:04:36.000 --> 00:04:40.290
So the goal here would be to also include layer 3

00:04:40.290 --> 00:04:43.000
and layer 4 in such tools.

00:04:43.000 --> 00:04:46.300
And actually this is exactly what we provide with

00:04:46.300 --> 00:04:48.000
cross-layer telemetry.

00:04:48.000 --> 00:04:52.080
And to reach such a goal, we first have to answer

00:04:52.080 --> 00:04:54.000
two key questions.

00:04:54.000 --> 00:04:57.600
The first one is how do we find a way to

00:04:57.600 --> 00:05:01.560
correlate the traces from the APM with the

00:05:01.560 --> 00:05:04.000
corresponding network traffic?

00:05:04.000 --> 00:05:07.230
So back to my example, if you want to trace

00:05:07.230 --> 00:05:10.000
exactly your database lookup in your code,

00:05:10.000 --> 00:05:14.000
you want to match the trace generated by the APM

00:05:14.000 --> 00:05:18.040
with the network traffic, which is the DB lookup

00:05:18.040 --> 00:05:19.000
on the link.

00:05:19.000 --> 00:05:22.000
And for that, we will use IOM.

00:05:22.000 --> 00:05:25.190
So IOM is just, it's in-situ operation

00:05:25.190 --> 00:05:28.000
administration and maintenance.

00:05:28.000 --> 00:05:32.730
It's actually used to carry some useful data in

00:05:32.730 --> 00:05:34.000
packets.

00:05:34.000 --> 00:05:36.720
We have developed it in the kernel and it should

00:05:36.720 --> 00:05:38.000
be available soon.

00:05:38.000 --> 00:05:40.000
Why do we use IOM?

00:05:40.000 --> 00:05:44.000
Well, we want to kill two birds with one stone.

00:05:44.000 --> 00:05:46.940
So as I said, it can carry a lot of useful low-level

00:05:46.940 --> 00:05:48.000
information.

00:05:48.000 --> 00:05:51.360
So for instance, you have the queue size, the

00:05:51.360 --> 00:05:53.000
buffer location,

00:05:53.000 --> 00:05:56.830
IDs of nodes and interfaces from where the packet

00:05:56.830 --> 00:06:01.000
is coming from and where it's going to,

00:06:01.000 --> 00:06:03.000
and so on and so on.

00:06:03.000 --> 00:06:06.510
And on the other side, we just enhanced the IOM

00:06:06.510 --> 00:06:14.000
header to carry both the trace ID and the span ID.

00:06:14.000 --> 00:06:17.150
So remember what I told you about the common

00:06:17.150 --> 00:06:19.000
point of those tools.

00:06:19.000 --> 00:06:23.000
They all have the same notion of trace and span.

00:06:23.000 --> 00:06:25.560
So a trace ID and a span ID both represent a

00:06:25.560 --> 00:06:27.000
unique ID of a span.

00:06:27.000 --> 00:06:31.000
So that's why we just carry the two.

00:06:31.000 --> 00:06:35.000
So the first important question is answered.

00:06:35.000 --> 00:06:37.000
Let's face the second one.

00:06:37.000 --> 00:06:41.000
When and how should we inject these IDs?

00:06:41.000 --> 00:06:43.000
So when?

00:06:43.000 --> 00:06:46.610
Well, we have two possibilities, either at socket

00:06:46.610 --> 00:06:49.000
creation or when sending data.

00:06:49.000 --> 00:06:51.430
So if you think a little bit about it, at socket

00:06:51.430 --> 00:06:54.000
creation, it wouldn't be enough.

00:06:54.000 --> 00:06:55.000
Why?

00:06:55.000 --> 00:06:58.260
Well, because an operator could use the same

00:06:58.260 --> 00:07:01.000
socket for all connections all along,

00:07:01.000 --> 00:07:04.420
or you could have for the same connection,

00:07:04.420 --> 00:07:06.000
multiple trace.

00:07:06.000 --> 00:07:08.330
See, if you want to monitor different part of

00:07:08.330 --> 00:07:10.270
your code, you would have multiple trace on the

00:07:10.270 --> 00:07:11.000
same socket.

00:07:11.000 --> 00:07:13.680
So that's not an option to inject those IDs at

00:07:13.680 --> 00:07:15.000
socket creation.

00:07:15.000 --> 00:07:18.970
And moreover, you don't want to modify the C

00:07:18.970 --> 00:07:21.130
library, because you would have to modify also

00:07:21.130 --> 00:07:22.000
high level languages.

00:07:22.000 --> 00:07:23.000
That's not an option.

00:07:23.000 --> 00:07:27.690
We want to provide a tool, an improvement to

00:07:27.690 --> 00:07:31.000
those tools, which is not a burden.

00:07:31.000 --> 00:07:33.460
So you want to integrate it easily without

00:07:33.460 --> 00:07:35.000
changing everything.

00:07:35.000 --> 00:07:39.560
So we will just select the option of injecting

00:07:39.560 --> 00:07:43.000
those IDs when we send the data.

00:07:43.000 --> 00:07:45.000
But now how?

00:07:45.000 --> 00:07:47.000
So again, we have several possibilities.

00:07:47.000 --> 00:07:52.230
The first one that comes in mind is to modify the

00:07:52.230 --> 00:07:57.000
send functions offered by the C library.

00:07:57.000 --> 00:07:59.260
But again, you don't want, as I said, you don't

00:07:59.260 --> 00:08:01.720
want to modify everything, because if you modify

00:08:01.720 --> 00:08:02.000
this,

00:08:02.000 --> 00:08:04.890
you would have to modify also high level

00:08:04.890 --> 00:08:06.000
languages.

00:08:06.000 --> 00:08:09.000
So not an option.

00:08:09.000 --> 00:08:11.000
So we are left with two other possibilities,

00:08:11.000 --> 00:08:15.870
which is to add a new syscall or use a netlink

00:08:15.870 --> 00:08:17.000
call.

00:08:17.000 --> 00:08:21.470
Again, if you add a new syscall, syscalls are not

00:08:21.470 --> 00:08:23.000
always portable.

00:08:23.000 --> 00:08:26.460
And the preferred way of doing this is usually to

00:08:26.460 --> 00:08:28.000
use a netlink call.

00:08:28.000 --> 00:08:30.200
So from a kernel perspective, it's always the

00:08:30.200 --> 00:08:31.000
best option.

00:08:31.000 --> 00:08:36.000
So we just select the netlink call as an option.

00:08:36.000 --> 00:08:39.800
So let me explain you a bit this architecture of

00:08:39.800 --> 00:08:42.000
cross-layer telemetry.

00:08:42.000 --> 00:08:46.280
So first of all, we have a client, which is a Jaeger

00:08:46.280 --> 00:08:48.000
client in this case,

00:08:48.000 --> 00:08:52.990
which is used to add tracing code to your

00:08:52.990 --> 00:08:55.000
application.

00:08:55.000 --> 00:08:57.810
And actually, when a trace is available, it will

00:08:57.810 --> 00:09:01.350
be sent to the agent, which will forward to the

00:09:01.350 --> 00:09:02.000
collector,

00:09:02.000 --> 00:09:05.390
which will apply some action on it and then store

00:09:05.390 --> 00:09:07.000
it in the database.

00:09:07.000 --> 00:09:11.710
Now we had a CLT library, which is also a client

00:09:11.710 --> 00:09:13.000
library.

00:09:13.000 --> 00:09:17.860
We had the netlink call. And we add an IOM agent

00:09:17.860 --> 00:09:20.000
and an IOM collector.

00:09:20.000 --> 00:09:23.620
So the CLT library is just an abstraction of the

00:09:23.620 --> 00:09:25.000
netlink call.

00:09:25.000 --> 00:09:28.470
So the application will just pass those IDs to

00:09:28.470 --> 00:09:30.000
the CLT library.

00:09:30.000 --> 00:09:33.580
So those IDs will be injected through a netlink

00:09:33.580 --> 00:09:36.000
call and they will be injected where?

00:09:36.000 --> 00:09:39.000
Well, to the socket.

00:09:39.000 --> 00:09:42.230
So again, someone will tell, hey, this is a per-socket

00:09:42.230 --> 00:09:43.000
solution.

00:09:43.000 --> 00:09:46.000
So what if you have some congestion in your queue?

00:09:46.000 --> 00:09:48.520
Maybe that one packet will be marked with those

00:09:48.520 --> 00:09:50.000
IDs and it shouldn't be.

00:09:50.000 --> 00:09:53.600
Well, you're correct. And we are currently

00:09:53.600 --> 00:09:58.110
working on a perfect solution, which will be per

00:09:58.110 --> 00:09:59.000
packet.

00:09:59.000 --> 00:10:03.000
So it comes with a lot of other challenges.

00:10:03.000 --> 00:10:06.080
But I think we are pretty close to a perfect

00:10:06.080 --> 00:10:07.000
solution.

00:10:07.000 --> 00:10:09.000
But right now, this one is working pretty well,

00:10:09.000 --> 00:10:14.000
except in the corner cases I mentioned.

00:10:14.000 --> 00:10:18.410
But again, the main goal of this one is to have

00:10:18.410 --> 00:10:21.000
some useful info to debug.

00:10:21.000 --> 00:10:25.100
So from layer 3, layer 4, as long as it's low

00:10:25.100 --> 00:10:26.000
level.

00:10:26.000 --> 00:10:28.530
It's not that important that you have a perfect

00:10:28.530 --> 00:10:31.000
match between the connection and traces.

00:10:31.000 --> 00:10:34.200
If you want a perfect match, then stay tuned and

00:10:34.200 --> 00:10:38.050
we will post the next version, which will be a

00:10:38.050 --> 00:10:41.000
lot more precise.

00:10:41.000 --> 00:10:45.000
So back to our architecture.

00:10:45.000 --> 00:10:49.000
When there is a request, so here, HTTPS request

00:10:49.000 --> 00:10:51.370
or it could be whatever it is, as long as it's

00:10:51.370 --> 00:10:52.000
network traffic.

00:10:52.000 --> 00:10:58.360
So when the packet goes out of the interface, it

00:10:58.360 --> 00:11:02.500
will be marked with the IDs that were injected

00:11:02.500 --> 00:11:03.000
here.

00:11:03.000 --> 00:11:06.290
And so actually, the application or it's not

00:11:06.290 --> 00:11:10.390
important if it's an application, as long as it's

00:11:10.390 --> 00:11:12.000
a node on the other side.

00:11:12.000 --> 00:11:15.330
And you have an IOM agent that will run on this

00:11:15.330 --> 00:11:16.000
part.

00:11:16.000 --> 00:11:21.030
And this one is responsible for gathering IOM

00:11:21.030 --> 00:11:22.000
data.

00:11:22.000 --> 00:11:26.000
And it will forward those to the IOM collector.

00:11:26.000 --> 00:11:29.680
And this guy will just correlate IOM data and any

00:11:29.680 --> 00:11:33.590
data that you want from layer 3 and layer 4 with

00:11:33.590 --> 00:11:37.000
the IDs that you have passed.

00:11:37.000 --> 00:11:40.000
And so it will send this to the IOM collector.

00:11:40.000 --> 00:11:42.000
So this is a correlation request.

00:11:42.000 --> 00:11:45.240
And so the IOM collector will be responsible for

00:11:45.240 --> 00:11:49.000
just storing the correlation inside the database.

00:11:49.000 --> 00:11:53.390
And actually, as a result, you can see directly

00:11:53.390 --> 00:11:57.990
in the Jaeger visualization IOM data or layer 3

00:11:57.990 --> 00:12:01.000
or 4 data as you want, as I said.

00:12:01.000 --> 00:12:04.110
So back to our example, you have the application

00:12:04.110 --> 00:12:05.000
database.

00:12:05.000 --> 00:12:09.700
We introduced a simulated congestion here on this

00:12:09.700 --> 00:12:12.000
guy on its interface.

00:12:12.000 --> 00:12:15.000
And as you can see, so this is the first node.

00:12:15.000 --> 00:12:17.000
So this is the app.

00:12:17.000 --> 00:12:19.000
This is the second node and third node.

00:12:19.000 --> 00:12:21.000
So the second node is this one.

00:12:21.000 --> 00:12:23.770
And you can see that the egress queue is

00:12:23.770 --> 00:12:24.000
increasing.

00:12:24.000 --> 00:12:28.000
So if you are the operator, now that you see this,

00:12:28.000 --> 00:12:31.000
well, you directly find root cause analysis.

00:12:31.000 --> 00:12:32.660
So you can see, OK, there is a problem in the

00:12:32.660 --> 00:12:33.000
queue.

00:12:33.000 --> 00:12:35.000
Maybe I could rebalance it.

00:12:35.000 --> 00:12:40.250
Or you are now capable of applying some actions

00:12:40.250 --> 00:12:43.000
to solve the problem.

00:12:43.000 --> 00:12:46.110
But again, without cross-layer geometry, you

00:12:46.110 --> 00:12:48.000
wouldn't have those data.

00:12:48.000 --> 00:12:50.010
So the only thing you would know is that it's

00:12:50.010 --> 00:12:51.000
slow, once again.

00:12:51.000 --> 00:12:55.000
OK.

00:12:55.000 --> 00:12:58.000
So let me conclude on this talk.

00:12:58.000 --> 00:12:59.520
I definitely think that it's a hot topic in the

00:12:59.520 --> 00:13:00.000
industry.

00:13:00.000 --> 00:13:03.880
So I've heard that there is a lot, a lot of

00:13:03.880 --> 00:13:06.000
interest on this one.

00:13:06.000 --> 00:13:09.550
And I do believe that CLT solves a lot of

00:13:09.550 --> 00:13:13.000
challenges in the tracing world.

00:13:13.000 --> 00:13:17.000
We are still working on some part to improve it.

00:13:17.000 --> 00:13:20.120
So I mentioned earlier another version which will

00:13:20.120 --> 00:13:21.000
be per packet.

00:13:21.000 --> 00:13:24.760
So to have a perfect correlation and match

00:13:24.760 --> 00:13:26.000
solution.

00:13:26.000 --> 00:13:29.860
And there is -- there are also some other things

00:13:29.860 --> 00:13:33.000
to improve, but not that important.

00:13:33.000 --> 00:13:35.400
So I insist on the fact that this solution is

00:13:35.400 --> 00:13:36.000
working.

00:13:36.000 --> 00:13:38.920
So this is something that you could use right now,

00:13:38.920 --> 00:13:40.000
if you want.

00:13:40.000 --> 00:13:43.000
And there is a link to the GitHub repo.

00:13:43.000 --> 00:13:46.000
So feel free to have a look at it.

00:13:46.000 --> 00:13:49.000
There is also a video to demonstrate how it works.

00:13:49.000 --> 00:13:51.000
Everything is explained here, there.

00:13:51.000 --> 00:13:55.000
So feel free to visit it.

00:13:55.000 --> 00:13:57.000
And that's it for me.

00:13:57.000 --> 00:13:59.000
Thank you very much for your attention.

00:13:59.000 --> 00:14:01.180
Feel free to contact me on my e-mail address or

00:14:01.180 --> 00:14:05.000
you can ask me a question right after this.

00:14:05.000 --> 00:14:06.000
Thanks again.

